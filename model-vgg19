# Import ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ
import os
from google.colab import drive

# 1. Mount Google Drive
print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Drive...")
drive.mount('/content/drive')

# 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ
project_path = '/content/coconut_project'
os.makedirs(project_path, exist_ok=True)

print("‚úÖ Setup ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
print(f"üìÅ ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ: {project_path}")

------------------------------------------------------------------------------------------------------------

import os
import shutil
import random
from math import floor

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î paths
source_path = '/content/drive/MyDrive/image'
output_path = '/content/data_each_state'

random.seed(42)

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (‡∏Å‡∏±‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥)
def safe_copy(src, dst_dir, prefix=""):
    filename = prefix + os.path.basename(src)
    dst_path = os.path.join(dst_dir, filename)
    i = 1
    while os.path.exists(dst_path):
        name, ext = os.path.splitext(filename)
        dst_path = os.path.join(dst_dir, f"{name}_{i}{ext}")
        i += 1
    shutil.copy2(src, dst_path)

# ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°
if os.path.exists(output_path):
    print(f"üóëÔ∏è ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà {output_path} ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
    shutil.rmtree(output_path)

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°
main_classes = ['stage_4', 'stage_5', 'stage_6']
other_classes = ['stage_1', 'stage_2', 'stage_3']

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
print("\nüìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå...")
for split in ['train', 'val', 'test']:
    os.makedirs(os.path.join(output_path, split), exist_ok=True)
    for cls in main_classes:
        os.makedirs(os.path.join(output_path, split, cls), exist_ok=True)
    os.makedirs(os.path.join(output_path, split, 'other'), exist_ok=True)

# ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å stage 1,2,3 ‡πÄ‡∏õ‡πá‡∏ô other
print("\nüîÑ ‡∏£‡∏ß‡∏° stage 1,2,3 ‡πÄ‡∏õ‡πá‡∏ô 'other'...")
other_files = []
for stage in other_classes:
    stage_path = os.path.join(source_path, stage)
    if os.path.exists(stage_path):
        files = [f for f in os.listdir(stage_path)
                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        for f in files:
            other_files.append((stage, f, os.path.join(stage_path, f)))
        print(f"  ‚úì {stage}: {len(files)} ‡∏£‡∏π‡∏õ")

# ‡∏™‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡πà‡∏á other
random.shuffle(other_files)
total_other = len(other_files)
other_n_train = floor(total_other * 0.7)
other_n_val = floor(total_other * 0.15)
other_n_test = total_other - other_n_train - other_n_val

print(f"\nüìä ‡∏Ñ‡∏•‡∏≤‡∏™ 'other' ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_other} ‡∏£‡∏π‡∏õ")
print(f"   ‚Üí Train {other_n_train}, Val {other_n_val}, Test {other_n_test}")

# ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå other ‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
for i, (stage, filename, filepath) in enumerate(other_files[:other_n_train]):
    dst_dir = os.path.join(output_path, 'train', 'other')
    safe_copy(filepath, dst_dir, prefix=f"{stage}_")

for i, (stage, filename, filepath) in enumerate(other_files[other_n_train:other_n_train+other_n_val]):
    dst_dir = os.path.join(output_path, 'val', 'other')
    safe_copy(filepath, dst_dir, prefix=f"{stage}_")

for i, (stage, filename, filepath) in enumerate(other_files[other_n_train+other_n_val:]):
    dst_dir = os.path.join(output_path, 'test', 'other')
    safe_copy(filepath, dst_dir, prefix=f"{stage}_")

# ‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå main classes (stage_4,5,6)
print("\nüîÑ ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• stage 4,5,6 ...")
summary = {}

for class_name in main_classes:
    class_path = os.path.join(source_path, class_name)

    if not os.path.exists(class_path):
        print(f"  ‚úó ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå {class_name}, ‡∏Ç‡πâ‡∏≤‡∏°...")
        continue

    files = [f for f in os.listdir(class_path)
             if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    random.shuffle(files)

    total = len(files)
    n_train = floor(total * 0.7)
    n_val = floor(total * 0.15)
    n_test = total - n_train - n_val

    print(f"  ‚úì {class_name}: {total} ‡∏£‡∏π‡∏õ ‚Üí Train {n_train}, Val {n_val}, Test {n_test}")

    # ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
    for f in files[:n_train]:
        safe_copy(os.path.join(class_path, f),
                  os.path.join(output_path, 'train', class_name))

    for f in files[n_train:n_train+n_val]:
        safe_copy(os.path.join(class_path, f),
                  os.path.join(output_path, 'val', class_name))

    for f in files[n_train+n_val:]:
        safe_copy(os.path.join(class_path, f),
                  os.path.join(output_path, 'test', class_name))

    summary[class_name] = {'train': n_train, 'val': n_val, 'test': n_test, 'total': total}

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏Ç‡∏≠‡∏á other
summary['other'] = {'train': other_n_train, 'val': other_n_val, 'test': other_n_test, 'total': total_other}

# ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
print("\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™")
print(f"{'Class':<12} {'Train':>7} {'Val':>6} {'Test':>6} {'Total':>7}")
print("-" * 40)
for cls in sorted(summary.keys()):
    d = summary[cls]
    print(f"{cls:<12} {d['train']:>7} {d['val']:>6} {d['test']:>6} {d['total']:>7}")

print(f"\n‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!")
print(f"üìÅ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà: {output_path}")
print(f"üí° ‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: {', '.join(sorted(summary.keys()))}")

---------------------------------------------------------------------------------------------------------- 
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG19 # Import VGG19
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import callbacks
import os
import math
import matplotlib.pyplot as plt
import numpy as np
import collections
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report


# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 0: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô ---
# ------------------------------------------------
print("--- ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Directory ---")
data_dir = '/content/data_each_state'

train_path = os.path.join(data_dir, 'train')
val_path = os.path.join(data_dir, 'val')
test_path = os.path.join(data_dir, 'test')

if not os.path.exists(data_dir):
    print(f"‚ùå ERROR: ‡πÑ‡∏°‡πà‡∏û‡∏ö Directory ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà: {data_dir}")
    print("‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß")
else:
    print(f"‚úÖ ‡∏û‡∏ö Directory ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà: {data_dir}")

    print("\n--- ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î ---")
    train_count = sum([len(files) for r, d, files in os.walk(train_path)])
    val_count = sum([len(files) for r, d, files in os.walk(val_path)])
    test_count = sum([len(files) for r, d, files in os.walk(test_path)])

    print(f"‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô train/: {train_count} ‡∏£‡∏π‡∏õ")
    print(f"‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô val/: {val_count} ‡∏£‡∏π‡∏õ")
    print(f"‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô test/: {test_count} ‡∏£‡∏π‡∏õ")

    print("\n--- ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Directory ‡∏Ç‡∏≠‡∏á Train ---")
    for root, dirs, files in os.walk(train_path):
        level = root.replace(train_path, '').count(os.sep)
        indent = ' ' * 4 * (level)
        print(f'{indent}{os.path.basename(root)}/ ({len(files)} files)')
        for d in dirs:
            print(f'{indent}    {d}/')

    print("\n--- ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Directory ‡∏Ç‡∏≠‡∏á Validation ---")
    for root, dirs, files in os.walk(val_path):
        level = root.replace(val_path, '').count(os.sep)
        indent = ' ' * 4 * (level)
        print(f'{indent}{os.path.basename(root)}/ ({len(files)} files)')
        for d in dirs:
            print(f'{indent}    {d}/')

    print("\n--- ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Directory ‡∏Ç‡∏≠‡∏á Test ---")
    for root, dirs, files in os.walk(test_path):
        level = root.replace(test_path, '').count(os.sep)
        indent = ' ' * 4 * (level)
        print(f'{indent}{os.path.basename(root)}/ ({len(files)} files)')
        for d in dirs:
            print(f'{indent}    {d}/')


# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ ImageDataGenerator ---
# ----------------------------------------------------------------
IMAGE_SIZE = (224, 224) # VGG19 ‡πÉ‡∏ä‡πâ 224x224 ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
BATCH_SIZE = 32

# Data Augmentation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∏‡∏î Train
train_datagen = ImageDataGenerator(
    rescale=1./255, # Normalize pixel values to [0, 1]
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Rescaling ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∏‡∏î Validation ‡πÅ‡∏•‡∏∞ Test (‡πÑ‡∏°‡πà‡∏ó‡∏≥ Augmentation)
val_test_datagen = ImageDataGenerator(rescale=1./255)

print("\n‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î Train ‡∏î‡πâ‡∏ß‡∏¢ ImageDataGenerator...")
train_generator = train_datagen.flow_from_directory(
    os.path.join(data_dir, 'train'),
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=True
)
print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î Train ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

print("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î Validation ‡∏î‡πâ‡∏ß‡∏¢ ImageDataGenerator...")
validation_generator = val_test_datagen.flow_from_directory(
    os.path.join(data_dir, 'val'),
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)
print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î Validation ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

print("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î Test ‡∏î‡πâ‡∏ß‡∏¢ ImageDataGenerator...")
test_generator = val_test_datagen.flow_from_directory(
    os.path.join(data_dir, 'test'),
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)
print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î Test ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

print("\nüí° ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡πÇ‡∏î‡∏¢ ImageDataGenerator:")
print(train_generator.class_indices)
num_classes = len(train_generator.class_indices)
print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {num_classes}")

print(f"\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Generator:")
print(f"  - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û Train: {train_generator.samples}")
print(f"  - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û Validation: {validation_generator.samples}")
print(f"  - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û Test: {test_generator.samples}")
print(f"  - ‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: {IMAGE_SIZE}")
print(f"  - Batch Size: {BATCH_SIZE}")


# --- 1.1 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô Validation Set ---
# -------------------------------------------------------
val_class_indices = validation_generator.classes
val_class_counts = collections.Counter(val_class_indices)
print("\n--- ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô Validation Set (‡∏à‡∏≤‡∏Å Generator) ---")
for class_idx, count in val_class_counts.items():
    class_name = list(train_generator.class_indices.keys())[list(train_generator.class_indices.values()).index(class_idx)]
    print(f"  {class_name}: {count} ‡∏£‡∏π‡∏õ ({count/validation_generator.samples*100:.2f}%)")

# --- 1.2 ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å Generator ---
# -----------------------------------------
print("\n--- ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å Train Generator ---")
try:
    sample_batch_images, sample_batch_labels = next(train_generator)
    print(f"Shape ‡∏Ç‡∏≠‡∏á Batch ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: {sample_batch_images.shape}")
    print(f"Shape ‡∏Ç‡∏≠‡∏á Batch Labels: {sample_batch_labels.shape}")
    print(f"‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î/‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Pixel: {sample_batch_images.min():.2f} / {sample_batch_images.max():.2f}")

    plt.figure(figsize=(12, 12))
    for i in range(min(25, sample_batch_images.shape[0])):
        ax = plt.subplot(5, 5, i + 1)
        plt.imshow(sample_batch_images[i])
        predicted_class_idx = np.argmax(sample_batch_labels[i])
        class_name = list(train_generator.class_indices.keys())[list(train_generator.class_indices.values()).index(predicted_class_idx)]
        plt.title(f"Class: {class_name}", fontsize=10)
        plt.axis("off")
    plt.tight_layout()
    plt.show()

    print("\n--- ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å Validation Generator ---")
    val_sample_batch_images, val_sample_batch_labels = next(validation_generator)
    print(f"Shape ‡∏Ç‡∏≠‡∏á Validation Batch ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: {val_sample_batch_images.shape}")
    print(f"‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î/‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Validation Pixel: {val_sample_batch_images.min():.2f} / {val_sample_batch_images.max():.2f}")

    plt.figure(figsize=(12, 12))
    for i in range(min(25, val_sample_batch_images.shape[0])):
        ax = plt.subplot(5, 5, i + 1)
        plt.imshow(val_sample_batch_images[i])
        predicted_class_idx = np.argmax(val_sample_batch_labels[i])
        class_name = list(validation_generator.class_indices.keys())[list(validation_generator.class_indices.values()).index(predicted_class_idx)]
        plt.title(f"Class: {class_name}", fontsize=10)
        plt.axis("off")
    plt.tight_layout()
    plt.show()

except Exception as e:
    print(f"‚ùå ERROR: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏î‡πâ: {e}")
    print("‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Generator ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á Batch")


# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN ‡∏î‡πâ‡∏ß‡∏¢ VGG19 (Base Model) ---
# ---------------------------------------------------------------
print("\n--- ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• VGG19 (Softmax Output) ---")
print("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• VGG19 (Pre-trained on ImageNet)...")
# ‡πÇ‡∏´‡∏•‡∏î VGG19 ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° Top (‡∏™‡πà‡∏ß‡∏ô Classification Head ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á ImageNet)
# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Classification Head ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
base_model = VGG19(weights='imagenet', include_top=False, input_shape=IMAGE_SIZE + (3,))
print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• VGG19 ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

print("‚ùÑÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á Freeze Layers ‡∏Ç‡∏≠‡∏á VGG19 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÉ‡∏ô‡∏£‡∏≠‡∏ö Warm-up...")
for layer in base_model.layers:
    layer.trainable = False
print("‚úÖ Freeze Layers ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

print("\nüõ†Ô∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Classification Head ‡πÉ‡∏´‡∏°‡πà...")
x = base_model.output
x = Flatten()(x) # ‡πÉ‡∏ä‡πâ Flatten ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö VGG19
x = Dense(256, activation='relu')(x) # Dense Layer ‡∏Ç‡∏ô‡∏≤‡∏î 256 Neurons
x = Dropout(0.5)(x) # ‡πÉ‡∏ä‡πâ Dropout 0.5 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö VGG (‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô)
predictions = Dense(num_classes, activation='softmax')(x) # *** Output Layer ‡πÉ‡∏ä‡πâ Softmax (‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á) ***

model = Model(inputs=base_model.input, outputs=predictions)
print("‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Classification Head ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Compile ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å (Warm-up) ---
# -----------------------------------------------------------
print("\n‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á Compile ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å (Warm-up)...")
# Learning Rate ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Warm-up
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
print("‚úÖ Compile ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

print("\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏Å‡πà‡∏≠‡∏ô Fine-tuning):")
model.summary()


# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å (Warm-up Training) ---
# -------------------------------------------------
print("\nüöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å (Train head only - Warm-up)...")
EPOCHS_WARMUP = 10 # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Epochs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏≠‡∏£‡πå‡∏°‡∏≠‡∏±‡∏û
history_warmup = model.fit(
    train_generator,
    steps_per_epoch=math.ceil(train_generator.samples / BATCH_SIZE),
    epochs=EPOCHS_WARMUP,
    validation_data=validation_generator,
    validation_steps=math.ceil(validation_generator.samples / BATCH_SIZE)
)
print("\n‚úÖ ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")


# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Fine-tuning Base Model ‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡∏ï‡πà‡∏≠ ---
# -------------------------------------------------

# Unfreeze Layers ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö VGG19
UNFREEZE_LAYERS_COUNT = 8 # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ 6-10 Layers ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö VGG19
print(f"\nüßä ‡∏Å‡∏≥‡∏•‡∏±‡∏á Unfreeze {UNFREEZE_LAYERS_COUNT} Layers ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Base Model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-tuning...")
for layer in base_model.layers[-UNFREEZE_LAYERS_COUNT:]:
    layer.trainable = True

print("\n‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Layer ‡∏ó‡∏µ‡πà trainable ‡∏´‡∏•‡∏±‡∏á Unfreeze:")
for layer in model.layers:
    print(f"{layer.name}: {layer.trainable}")

# Compile ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å Unfreeze Layers ‡∏î‡πâ‡∏ß‡∏¢ Learning Rate ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏•‡∏á‡∏°‡∏≤‡∏Å
NEW_LEARNING_RATE = 0.000005 # Learning Rate ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-tuning (5e-6)
print(f"\n‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á Compile ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Fine-tuning (Learning Rate: {NEW_LEARNING_RATE})...")
model.compile(optimizer=Adam(learning_rate=NEW_LEARNING_RATE),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
print("‚úÖ Compile ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Fine-tuning ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

print("\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏´‡∏•‡∏±‡∏á Fine-tuning Unfreeze ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô):")
model.summary()


# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Callbacks
early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True
)

model_checkpoint_path = 'best_coconut_vgg19_softmax_model.keras' # ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
model_checkpoint = callbacks.ModelCheckpoint(
    filepath=model_checkpoint_path,
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
)

# ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢ Fine-tuning
print("\nüöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• (Fine-tuning VGG19 with Softmax)...")
EPOCHS_FINE_TUNE = 70 # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Epochs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-tuning
history_fine_tune = model.fit(
    train_generator,
    steps_per_epoch=math.ceil(train_generator.samples / BATCH_SIZE),
    epochs=EPOCHS_FINE_TUNE,
    validation_data=validation_generator,
    validation_steps=math.ceil(validation_generator.samples / BATCH_SIZE),
    callbacks=[early_stopping, model_checkpoint]
)
print("\n‚úÖ ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• (Fine-tuning VGG19 with Softmax) ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
print(f"üåü ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: {model_checkpoint_path}")


# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ---
# -----------------------------------------------
print("\nüìà ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...")

acc = history_warmup.history['accuracy'] + history_fine_tune.history['accuracy']
val_acc = history_warmup.history['val_accuracy'] + history_fine_tune.history['val_accuracy']

loss = history_warmup.history['loss'] + history_fine_tune.history['loss']
val_loss = history_warmup.history['val_loss'] + history_fine_tune.history['val_loss']

total_epochs = EPOCHS_WARMUP + len(history_fine_tune.history['accuracy'])
epochs_range = range(total_epochs)

plt.figure(figsize=(14, 6))

# ‡∏Å‡∏£‡∏≤‡∏ü Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.plot([EPOCHS_WARMUP-0.5, EPOCHS_WARMUP-0.5], [min(min(acc), min(val_acc)), max(max(acc), max(val_acc))], 'k--', label='Fine-tune Starts')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy (Softmax)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.grid(True)

# ‡∏Å‡∏£‡∏≤‡∏ü Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.plot([EPOCHS_WARMUP-0.5, EPOCHS_WARMUP-0.5], [min(min(loss), min(val_loss)), max(max(loss), max(val_loss))], 'k--', label='Fine-tune Starts')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss (Softmax)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)

plt.tight_layout()
plt.show()

print("\n‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß.")


# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ö‡∏ô‡∏ä‡∏∏‡∏î Test ---
# -------------------------------------------------------
print("\n--- ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ö‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö (Softmax) ---")

if not os.path.exists(model_checkpoint_path):
    print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà: {model_checkpoint_path}")
    print("‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß")
else:
    print(f"\n‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å: {model_checkpoint_path}")
    best_model = tf.keras.models.load_model(model_checkpoint_path)
    print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

    print("\nüìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö...")
    test_loss, test_accuracy = best_model.evaluate(
        test_generator,
        steps=math.ceil(test_generator.samples / BATCH_SIZE),
        verbose=1
    )

    print("\n--- ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ö‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö ---")
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {test_accuracy:.4f}")

    # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á (True Labels)
    y_true = test_generator.classes

    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•
    print("\n‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏ö‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Confusion Matrix...")
    y_pred_proba = best_model.predict(
        test_generator,
        steps=math.ceil(test_generator.samples / BATCH_SIZE),
        verbose=1
    )
    y_pred = tf.argmax(y_pred_proba, axis=1).numpy()

    class_labels = list(train_generator.class_indices.keys())
    print("‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™:", class_labels)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    print("\n--- Confusion Matrix ---")
    print(cm)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_labels, yticklabels=class_labels)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Classification Report
    print("\n--- Classification Report ---")
    print(classification_report(y_true, y_pred, target_names=class_labels))

    # --- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Accuracy ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™ --- #
per_class_accuracy = cm.diagonal() / cm.sum(axis=1)

print("\n--- Per-Class Accuracy ---")
for i, class_name in enumerate(class_labels):
    print(f"{class_name}: {per_class_accuracy[i]*100:.2f}%")

# ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏ó‡πà‡∏á Per-Class Accuracy
plt.figure(figsize=(8, 6))
sns.barplot(x=class_labels, y=per_class_accuracy*100, palette="viridis")
plt.ylabel("Accuracy (%)")
plt.xlabel("Class")
plt.title("Per-Class Accuracy")
plt.ylim(0, 100)
plt.show()
